# <small><small><small><small>CSE4120</small></small></small></small> Fundamentals Of Compiler Construction

ë³¸ ë¬¸ì„œëŠ” **23ë…„ë„ 2í•™ê¸° ì„œê°•ëŒ€í•™êµ ìµœì¬ìŠ¹ êµìˆ˜ë‹˜**ì˜ **CSE4120 ê¸°ì´ˆ ì»´íŒŒì¼ëŸ¬ êµ¬ì„±** ìˆ˜ì—…ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.

ìˆ˜ì—… ë‚´ìš©ê³¼ ê°•ì˜ìë£Œì— ë”í•´ ê°œì¸ì ì¸ í•´ì„ì´ ë”í•´ì§„ ì •ë¦¬ì´ë¯€ë¡œ ì˜¤ë¥˜ê°€ ì¡´ì¬í•  ìˆ˜ ìˆë‹¤ëŠ” ì  ì£¼ì˜í•´ì£¼ì„¸ìš”.<br/>
ë˜í•œ Proj #3ì˜ ê²½ìš° ì—­ëŸ‰ ë° ì‹œê°„ ë¶€ì¡±ìœ¼ë¡œ Copy Propogationê³¼ Common Subexpression Eliminationì„ êµ¬í˜„í•˜ì§€ ëª»í–ˆë‹¤ëŠ” ì  ì°¸ê³ í•´ì£¼ì„¸ìš” ğŸ¥²

í•´ë‹¹ ê³¼ì œëŠ” LLMì˜ ë„ì›€ ì—†ì´ êµ¬í˜„ëœ ë§Œí¼, ë¬´ë¶„ë³„í•œ ì¹´í”¼ëŠ” ë³´ë‹¤ ì‰½ê²Œ í™•ì¸ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.<br/>
ë” ë‚˜ì•„ê°€, ë³¸ ê³¼ì œì˜ ìˆ˜í–‰ì´ ìˆ˜ì—…ì˜ ë‚´ìš©ì„ ë”ìš± ê¹Šê²Œ ì´í•´í•˜ê³  ì†Œí™”í•˜ëŠ”ë° ì§ì ‘ì ìœ¼ë¡œ ë„ì›€ì´ ëœë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ë”°ë¼ì„œ í•´ë‹¹ ì½”ë“œë“¤ì€ ë‹¨ìˆœí•œ ì°¸ê³ ìš©ìœ¼ë¡œ í™œìš©í•˜ëŠ” ê²ƒì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤ ğŸ˜ƒ

<br/>

# <small><small><small><small>Assignment #1.</small></small></small></small> Flex & Bison Exercise

### Overview

The goal of this project is to define the specifications for automatic generation of lexer and parser.

**1. Lexer** <br/>
The **lexer** takes the program's source code as input, tokenizes it according to predefined rules, and rejects any malformed input. These rules specify the valid token patterns, typically expressed as **regular expressions (RegEx)**. The tool used to automatically generate the lexer, **Flex**, accepts RegEx-based specifications to produce the lexer. 

Our task is to define the token patterns in `prog.l` using Regex, so that Flex can generate a valid lexer. 

Additionally, the lexer generation process involves converting the provided RegEx into a **Non-Deterministic Finite Automata (NFA)**, which is then converted into a **Deterministic Finite Automata (DFA)**. A detailed explanation for this complex process is beyond the scope of this assignment, but you can refer to the lecture notes for more information.

**2. Parser** <br/>
The **parser** uses the token stream produced by the lexer to construct an **Abstract Syntax Tree (AST)** based on a set of grammar rules. These grammar rules define how the parser interprets the input and are described using a **Context-Free Grammar (CFG)**. The parser generation tool, **Bison**, accepts CFG and additional specifications to generate the parser.

Our task is to define the CFG and the AST construction logic in `prog.y` so that Bison can automatically generate the parser.

Additionally, there are two main methods for automatic parser generation: top-down and bottom-up. Both methods use complex algorithms to generate parsers. In this assignment, Bison uses the bottom-up approach to generate an LALR parser. As with the lexer, a detailed explanation of these algorithms is beyond the scope of this assignment, so please refer to the lecture notes for further details.

<!-- ### Details
* `run`
    * Receives a `program` and returns a error list that contains all the error occured during the type checking.
    * ì „ì²´ íƒ€ì… ì²´í‚¹ ì•Œê³ ë¦¬ì¦˜ì„ ì§„í–‰í•œë‹¤. ë¨¼ì € `collect_vars`ì™€ `collect_functions`ë¥¼ ìˆ˜í–‰í•˜ë©° global declarationê³¼ í•¨ìˆ˜ë¥¼ ìˆœíšŒí•˜ë©° global symbol tableì„ ì±„ìš´ë‹¤. ì´í›„ `check_functions`ë¥¼ í˜¸ì¶œí•´ ê° í•¨ìˆ˜ ë‚´ë¶€ë¥¼ í™•ì¸í•˜ë©° ì—ëŸ¬ë¥¼ ì—ëŸ¬ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•œë‹¤.

* `collect_vars` & `collect_functions`

    * ê° ê° global declarationê³¼ unctionì„ ìˆœíšŒí•˜ë©° ì´ë¦„ê³¼ íƒ€ì…ì„ symbol tableì— ê¸°ë¡í•œë‹¤.
    * ì´ë•Œ symbol tableì€ `String`ê³¼ `ctype_entry`ì˜ mapìœ¼ë¡œ íƒ€ì… ì €ì¥ ì‹œ `ctype_entry`ë¡œì˜ íƒ€ì… ë³€í™˜ì´ í•„ìš”í•˜ë‹¤. ì´ë¥¼ ìœ„í•´ì„œ `collect_functoins`ëŠ” `filter_decls_to_ctyps`ë¼ëŠ” helper functionì„ í™œìš©í•œë‹¤.

* `check_functions`
    * global variableê³¼ functionì˜ íƒ€ì…ì´ ì €ì¥ëœ symbol table, ê·¸ë¦¬ê³  í•¨ìˆ˜ë“¤ì„ receiveí•˜ê³  errorlistë¥¼ ë°˜í™˜í•œë‹¤.
    * `collect_vars`ë¥¼ í™œìš©í•´ function ë‚´ì˜ declarationì„ symbol tableì— ì¶”ê°€í•œë‹¤.
    * `check_statements`ë¥¼ í˜¸ì¶œí•˜ì—¬ errorlistë¥¼ ë°˜í™˜í•œë‹¤.

* `check_statements`

* `check_exp`

ê°œì¸ì ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ í•¨ìˆ˜ë„ ì¡´ì¬í•˜ê³  í•¨ìˆ˜í˜• ì–¸ì–´ë¥¼ ì˜ ë‹¤ë£¨ì§€ ëª»í•´ ì½”ë“œê°€ ê°€ë…ì„±ë„ ë‚®ê³  ê¸¸ì–´ì¡Œë‹¤. ğŸ¥² -->

<br/>

# <small><small><small><small>Assignment #2.</small></small></small></small> OCaml Exercise

Requires some simple `OCaml` implementations. Helps you to get familliar with `OCaml` and the concept of functional languages which is essential for the further projects ğŸª

Use the files in `OCaml-Example` as a cheat sheet ğŸ‘€

<br/>

# <small><small><small><small>Project #1.</small></small></small></small> Type Checking

### Overview

The goal of this project is to implement type checking for AST.

Type checking is a process that occurs in the front-end (FE) of the compiler, before IR generation, and is responsible for detecting obvious semantic errors from the AST. While syntax analysis (parsing) can catch syntax-level errors, such as tokens appearing in incorrect positions, it does not detect semantic errors. Type checking indentifies errors like type mismatches, use of undeclared identifiers, redeclaration of identifiers, and more. Note that there are still some semantic errors that type checking cannot detect.

If the source language uses implicit typing, such as python, type inference becomes necessary, making the process more complex than for explicitly typed languages. Fortunately, the target language in this assignment is a simplified version of C, which uses explicit typing, eliminating the need for type inference. For an example of a type inference system, refer to [CSE4050 Programming Languages](https://github.com/Goonco/sogang-cs-projects/tree/main/cse4050-programming-languages).

In explicitly typed languages, type checking is performed using a symbol table. A **Symbol Table** stores mappings between variables or function names and their associated types. The type checker uses this table to ensure variables are used correctly throughout the program.

The symbol table needs to maintain correct information among different scopes. Since each identifier has its own scope, the symbol table must be updated accordingly when entering new scopes, such as functions. Care should be taken to correctly manage the symbol table during scope transitions to ensure accurate type checking.

<br/>

# <small><small><small><small>Project #2.</small></small></small></small> IR Translation

### Overview

The goal of this project is to generate IR from an AST that has already undergone type checking.

**IR** stands for **Intermediate Representation**, and as the name suggests, it serves as an intermediate stage before the compiler converts the code into the target language. By introducing this intermediate stage, the compiler can support multiple source and target languages. Additionally, IR operates at a lower level of abstraction compared to the AST, making it more suitable for optimizations and machine code generation.

The key difference between AST and IR is that IR distinguishes variables (in memory) from registers. This means that for operands such as add/sub, IR needs to load variables from memory to registers. Later on, these IR registers are mapped to actual CPU registers, making it significantly easier to generate machine code compared to working directly with the AST. However, note that there remains a gap between IR and machine level code. For example, while IR assumes an infinite number of registers, real CPUs have a limited number.

Our task is to utilize the characteristics of IR to translate the AST into IR instructions as defined in `ir.ml`. To introduce the concept of registers, a symbol table is required to map variable names to their corresponding registers. Unlike in Project #1, Project #2 must also support 1D arrays, with the AST reflecting array structures. Since the size of an array index depends on its type, type information must also be stored in the symbol table. Additionally, this project must handle short-circuit evaluation for logical operators such as `OR` and `AND`.

During the IR generation process, you may encounter inefficiencies such as the generation of an excessive number of registers. While these inefficiencies could also be optimized in the IR generation code, they are typically addressed in the middle end of the compiler during IR optimization. Personally, I recommended to focus on creating readable and intuitive translation code, even if it means sacrificing some optimization at this stage.

<!-- ë¨¼ê°€ reigsterëŠ” ê³§ ë©”ëª¨ë¦¬ ë¼ëŠ” ê±¸ ì¢€ë” ë§í•˜ê³  ì‹¶ì€ë° ì˜ì•ˆë˜ë„¤ -->

<br/>

# <small><small><small><small>Project #3.</small></small></small></small> Optimization

### Overview

The goal of this project is IR optimization, or in other words, implementing the middle end of the compiler.

The purpose of IR optimization is to generate â€œbetter code.â€ While â€œbetter codeâ€ can have various interpretations, it typically refers to improvements such as reducing execution time, code size, and memory usage.

Also note that IR optimization must never alter the behavior of the original IR. It should only focus on enhancing the efficiency of the code.

While individual optimizations can be complex, the overall process in the compilerâ€™s middle end is quite straightforward: it simply receives the IR from the front end, applies optimizations, and outputs the optimized IR. In this project, we will perform five key optimizations on the provided IR:

* constant folding
* constant propagation
* copy propagation
* common subexpression elimination
* dead code elimination

<br/>

<h3 align="center">Thank you for reading, I hope you enjoy the project too ğŸ¤©</h3>