Q1. Why is it necessary to divide the data into batches
Batch란 사전적으로는 (일관적으로 처리되는) 집단, 무리를 의미한다. 이러한 배치는 training에 있어서 전체 데이터셋을 작은 그룹으로 나누는 것을 의미한다.
Training에 있어서 전체 데이터셋을 한 번에 메모리에 로드하는 방식과 데이터를 소분하여 처리하는 것 중 한가지를 선택할 수 있으며 데이터를 소분하는 것이 바로 배치를 통한 처리를 일컫는다. 이때 일반적으로 배치로 나누어 처리하는 것이 성능이 훨씬 좋으며 그 이유는 다음과 같다.

메모리 및 데이터 처리 효율성 측면에서 전체 데이터셋을 로드하고 처리하는 과정은 자원 소모가 크다. 또한 자원 소모의 효율성을 넘어 전체 데이터셋의 크기가 너무 크거나 GPU나 CPU의 성능이 좋지 않은 상황에서 전체 데이터셋을 메모리에 적재하는 것 자체가 불가능할 수 있다. 이러한 경우 반드시 배치를 통한 처리를 고려해야 한다. 또한 데이터를 배치로 처리할 경우 입력/출력 작업과 네트워크 오버헤드도 줄이고 병렬 처리를 통한 훈련 속도를 높일 수 있다는 장점이 있다.
다음으로 모델 훈련 및 최적화의 정확성과 안정성을 향상시킬 수 있다. 배치는 데이터의 다양성을 반영하며 각 배치가 모델의 가중치 업데이트에 기여하므로 모델이 전체 데이터셋에 대해 더 안정적으로 학습이 가능하다. 또한 데이터셋을 배치단위로 나누어 여러 번 가중치를 업데이트하여 불안정한 특정 데이터 샘플의 모델에 대한 영향도 줄일 수 있다.

그러나 배치 처리의 경우 단점도 존재한다. 배치 처리는 데이터를 처리할 때 소요되는 시간과 복잡성을 증가시킬 수 있고 또한 다음 배치로 넘어가기 전 전체 배치의 처리를 기다려야 하기 때문에 피드백 결과가 지연될 수도 있다.

Q2. What is an epoch
epoch는 전체 데이터 셋이 신경망을 통과하는 횟수, 다시 말해 모든 batch를 학습하는 횟수를 의미한다.
이러한 epoch는 모델 생성에 영향을 미치는 중요한 하이퍼 파라미터 중 하나이며 이전 실험에서 확인했듯이 적절한 epoch 수의 선택이 모델의 성능에 직접적인 영향을 미친다.
epoch수가 너무 작은 경우 모델은 훈련 데이터에 대한 충분한 학습을 하지 못하여 불안정하고 안 좋은 성능을 갖게 된다.
반대로 epoch수가 너무 증가하게 되면 모델은 훈련 데이터에 대한 학습을 더 많이 하게 되므로 모델이 해당 훈련 데이터에 대해서만 과하게 학습할 가능성이 있다. 이는 곧 오버피팅의 발생을 의미하며 특정 테스트에서는 좋은 성능을 가질 수 있지만 일반적인 경우에 대한 성능이 현저히 떨어지게 된다.
따라서 적절한 epoch 수를 선택하기 위해서는 epoch 수에 따른 모델의 성능을 꾸준히 모니터링하고 동시에 검증 데이터를 통해 일반화 성능을 평가하여 가장 적절한 epoch 수를 확인해야 한다.

Q3. What does the under train code do
for batch_idx, batch in enumerate(batches):

            # expand the batch
            batch_in.append(batch[0].to(device))
            batch_gt.append(batch[1].to(device))

            # when the batch size is enough, train it
            if (batch_idx + 1) % batch_size == 0 or batch_idx == len(batches) - 1:
                batch_in = torch.reshape(torch.cat(batch_in, dim=0),
                                         (-1, 96, 96, 3))
                batch_gt = torch.reshape(torch.cat(batch_gt, dim=0),
                                         (-1,))

                # computer forward inference, compute loss
                batch_out = infer_action(batch_in)
                loss = loss_function(batch_out, batch_gt)

먼저 데이터셋을 순회하며 배치를 가져오기 위해 루프를 돌며 미니 배치의 인덱스를 순회한다. 이후 현재 미니배치의 데이터와 라벨을 각 각 batch_in과 batch_gt리스트에 추가하고 데이터를 device로 전송한다.
이때 배치의 순회가 끝나거나 지정한 배치 크기에 도달한 경우 현재까지 저장한 미니배치를 사용하여 모델을 학습한다. 학습 전 batch_in과 batch_gt에 모인 미니배치를 PyTorch Tensor로 변환 및 모델 입력을 위한 데이터의 가공을 한다.
다음으로 신경망에 입력 데이터를 전달하여 출력을 얻으며 지정한 loss fucntion을 통해 손실을 계산한다. 해당 과정을 모든 배치를 순회할 때 까지 반복한다.
